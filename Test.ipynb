{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unity Env loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import keras\n",
    "from utils.memory_buffer import MemoryBuffer\n",
    "from utils.networks import tfSummary, OrnsteinUhlenbeckProcess\n",
    "from utils.stats import gather_stats\n",
    "from random import random, randrange\n",
    "# from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "# %matplotlib inline\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfig, EngineConfigurationChannel\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Game env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channel = EngineConfigurationChannel()\n",
    "env_name = \"3Dball//UnityEnvironment\"  # Name of the Unity environment binary to launch\n",
    "env = UnityEnvironment(env_name, worker_id=0,side_channels=[channel])\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit display setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel.set_configuration_parameters(time_scale = 3, width =300, height =300)\n",
    "env.reset()\n",
    "behavior_names = env.behavior_specs.keys()\n",
    "list(behavior_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get env basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step() \n",
    "group_name = list(env._env_specs.keys())[0]\n",
    "group_spec = env.behavior_specs[group_name]\n",
    "\n",
    "state_dim = group_spec.observation_shapes[0]\n",
    "if group_spec.action_spec.is_continuous():\n",
    "    action_dim = group_spec.action_spec.continuous_size\n",
    "else:\n",
    "    action_dim = group_spec.action_spec.discrete_branches[0]\n",
    "group_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDQN.ddqn import DDQN\n",
    "ddqn = DDQN(action_dim = action_dim, state_dim = state_dim, with_per=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = []\n",
    "\n",
    "cumul_reward, done  = 0, False\n",
    "env.reset()\n",
    "c = 0\n",
    "\n",
    "model_name = env_name.split('/')[0] + '_model.h5'\n",
    "target_model_name = env_name.split('/')[0] + '_target_model.h5'\n",
    "if os.path.isfile(model_name):\n",
    "    print('load model')\n",
    "    ddqn.agent.model.load_weights(model_name)\n",
    "\n",
    "tr = 0\n",
    "tf = 0\n",
    "while True:\n",
    "    # Get state information\n",
    "    decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "    old_state = decision_steps.obs[0]\n",
    "    obs_space = len(old_state) # Check obs space\n",
    "    if obs_space > 0:\n",
    "        # Actor picks an action (following the policy)\n",
    "        if random() <= ddqn.epsilon:\n",
    "#             a = group_spec.action_spec.random_action(obs_space)\n",
    "            \n",
    "            # only for grid world\n",
    "            a = np.random.randint(1,5,size = (obs_space,1))\n",
    "        else:\n",
    "            a = ddqn.policy_action(old_state)\n",
    "            a = np.reshape(a,(a.shape[0],1))\n",
    "#             print(a)\n",
    "        env.set_actions(group_name, a)\n",
    "#         plt.imshow(old_state[0])\n",
    "#         plt.show()\n",
    "        env.step()\n",
    "        # Retrieve new state, reward, and whether the state is terminal\n",
    "        decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "        \n",
    "        done = np.zeros((obs_space), dtype=bool)\n",
    "        done[terminal_steps.agent_id] = True\n",
    "        \n",
    "        new_state, r = decision_steps.obs[0], decision_steps.reward\n",
    "        if terminal_steps.reward.size>0:\n",
    "            r[terminal_steps.agent_id] = terminal_steps.reward\n",
    "#         plt.imshow(new_state[0])\n",
    "#         plt.show()\n",
    "#         print('-'*20)\n",
    "        # Memorize for experience replay\n",
    "        ddqn.memorize(old_state, a, r, done, new_state)\n",
    "        # Update current state\n",
    "        old_state = new_state\n",
    "        cumul_reward += r\n",
    "        if len(terminal_steps.reward) > 0:\n",
    "            for i in terminal_steps.reward:\n",
    "                if i > 0:\n",
    "                    tr+= 1\n",
    "                if i < 0:\n",
    "                    tf+= 1\n",
    "        # Train DDQN and transfer weights to target network\n",
    "        \n",
    "        if(ddqn.buffer.size() > ddqn.batch_size):\n",
    "#             print('Training')\n",
    "            ddqn.train_agent(ddqn.batch_size)\n",
    "            ddqn.agent.transfer_weights()\n",
    "        c+= 1\n",
    "        if c % 100 == 0:\n",
    "            print('reach goal: ', tr)\n",
    "            print('fail: ', tf)\n",
    "            tr, tf= 0,0\n",
    "            print(ddqn.epsilon)\n",
    "            ddqn.save(model_name)\n",
    " \n",
    "    else:\n",
    "        env.step()\n",
    "\n",
    "# Gather stats every episode for plotting\n",
    "if(ddqn.gather_stats):\n",
    "    mean, stdev = gather_stats(ddqn, env)\n",
    "    results.append([e, mean, stdev])\n",
    "\n",
    "# Export results for Tensorboard\n",
    "score = tfSummary('score', cumul_reward)\n",
    "summary_writer.add_summary(score, global_step=e)\n",
    "summary_writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDPG.ddpg import DDPG\n",
    "\n",
    "ddpg = DDPG(act_dim = action_dim, env_dim = state_dim, act_range = 1, buffer_size = 10000, batch_size = 300, lr = 0.0005)\n",
    "# from DDPG.ddpgOld import ActorCritic\n",
    "# import tensorflow as tf\n",
    "# import keras.backend as K\n",
    "# sess = tf.Session()\n",
    "# K.set_session(sess)\n",
    "# ddpg = ActorCritic(act_dim = action_dim, env_dim = state_dim, sess= sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.actor.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# import matplotlib.pyplot as plt\n",
    "# First, gather experience\n",
    "ddpg.epsilon = 1\n",
    "ddpg.epsilon_decay = 0.999999\n",
    "ddpg.batch_size = 300\n",
    "# Reset episode\n",
    "time, cumul_reward, done = 100000, 0, False\n",
    "old_state = env.reset()\n",
    "actions, states, rewards = [], [], []\n",
    "noise = OrnsteinUhlenbeckProcess(size=ddpg.act_dim)\n",
    "tr, tf, c = 0,0,0\n",
    "while True:\n",
    "    decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "    old_state = decision_steps.obs[0]\n",
    "    obs_space = len(old_state) # Check obs space\n",
    "    if obs_space > 0:\n",
    "    # Actor picks an action (following the deterministic policy)\n",
    "\n",
    "        a = ddpg.policy_action(old_state)\n",
    "        a = np.reshape(a,(a.shape[0],a.shape[1]))\n",
    "    # Clip continuous values to be valid w.r.t. environment\n",
    "#         print(a)\n",
    "        a = np.clip(a+noise.generate(time), -ddpg.act_range, ddpg.act_range)\n",
    "\n",
    "    # Retrieve new state, reward, and whether the state is terminal\n",
    "        env.set_actions(group_name, a)\n",
    "#         plt.imshow(old_state[0])\n",
    "#         plt.show()\n",
    "        env.step()\n",
    "        decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "        done = np.zeros((obs_space), dtype=bool)\n",
    "        done[terminal_steps.agent_id] = True\n",
    "        new_state, r = decision_steps.obs[0], decision_steps.reward\n",
    "\n",
    "        if terminal_steps.reward.size>0:\n",
    "            r = np.ones(12)*.1\n",
    "            r[terminal_steps.agent_id] = terminal_steps.reward\n",
    "#         plt.imshow(new_state[0])\n",
    "#         plt.show()\n",
    "#         print('-'*20)\n",
    "        # Memorize for experience replay\n",
    "#         print(r)\n",
    "        ddpg.memorize(old_state, a, r, done, new_state)\n",
    "        \n",
    "        # Sample experience from buffer\n",
    "        states, actions, rewards, dones, new_states, _ = ddpg.sample_batch(ddpg.batch_size)\n",
    "        # Predict target q-values using target networks\n",
    "        q_values = ddpg.critic.target_predict([new_states, ddpg.actor.target_predict(new_states)])\n",
    "        # Compute critic target\n",
    "        critic_target = ddpg.bellman(rewards, q_values, dones)\n",
    "        # Train both networks on sampled batch, update target networks\n",
    "        ddpg.update_models(states, actions, critic_target)\n",
    "            \n",
    "        # Update current state\n",
    "        old_state = new_state\n",
    "        cumul_reward += r\n",
    "        if len(terminal_steps.reward) > 0:\n",
    "            for i in terminal_steps.reward:\n",
    "                if i > 0:\n",
    "                    tr+= 1\n",
    "                if i < 0:\n",
    "                    tf+= 1\n",
    "        time += 1\n",
    "\n",
    "        c+= 1\n",
    "#         if c % 100 == 0:\n",
    "#             print('reach goal: ', tr)\n",
    "#             print('fail: ', tf)\n",
    "#             tr, tf= 0,0\n",
    "#             print(ddpg.epsilon)\n",
    "#             ddpg.save(model_name)\n",
    " \n",
    "    else:\n",
    "        env.step()\n",
    "\n",
    "# Gather stats every episode for plotting\n",
    "if(args.gather_stats):\n",
    "    mean, stdev = gather_stats(self, env)\n",
    "    results.append([e, mean, stdev])\n",
    "\n",
    "# Export results for Tensorboard\n",
    "score = tfSummary('score', cumul_reward)\n",
    "summary_writer.add_summary(score, global_step=e)\n",
    "summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "old_state = env.reset()\n",
    "for i in range(10000):\n",
    "#     time.sleep(0.1)\n",
    "    decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "#     print(decision_steps.action_mask  )\n",
    "#     print('t: ',terminal_steps.agent_id)\n",
    "#     print('d: ', decision_steps.reward)\n",
    "#     print(decision_steps.obs[0].shape)\n",
    "#     print('t: ',terminal_steps.obs[0].shape)\n",
    "#     print((terminal_steps.agent_id_to_index))\n",
    "#     print((terminal_steps.interrupted))\n",
    "\n",
    "    # Random moves\n",
    "#     a = len(decision_steps.obs[0])\n",
    "#     env.set_actions(group_name,group_spec.action_spec.random_action(a))\n",
    "    \n",
    "     \n",
    "    # Agent moves\n",
    "    old_state = decision_steps.obs[0]\n",
    "    obs_space = len(old_state)\n",
    "    if obs_space > 0:\n",
    "        a = ddpg.policy_action(old_state)\n",
    "\n",
    "        a = np.reshape(a,(a.shape[0],a.shape[1]))\n",
    "    #     a = np.reshape(a,(a.shape[0],1))\n",
    "        env.set_actions(group_name,a)\n",
    "    \n",
    "#     print(a)\n",
    "    # We send data to Unity : A string with the number of Agent at each\n",
    "    env.step()  # Move the simulation forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while len(decision_steps.reward) == 0:\n",
    "    env.step() \n",
    "    decision_steps, terminal_steps = env.get_steps(group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neat\n",
    "import numpy as np\n",
    "import gym\n",
    "import visualize\n",
    "# \n",
    "# GAME = 'CartPole-v0'\n",
    "# env = gym.make(GAME).unwrapped\n",
    "\n",
    "CONFIG = \"./config\"\n",
    "EP_STEP = 0             # maximum episode steps\n",
    "GENERATION_EP = 10      # evaluate by the minimum of 10-episode rewards\n",
    "TRAINING = True         # training or testing\n",
    "CHECKPOINT = 120        # test on this checkpoint\n",
    "\n",
    "\n",
    "def eval_genomes(genomes, config):\n",
    "    for genome_id, genome in genomes:\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        ep_r = []\n",
    "        for ep in range(GENERATION_EP): # run many episodes for the genome in case it's lucky\n",
    "            accumulative_r = 0.         # stage longer to get a greater episode reward\n",
    "            \n",
    "            agent_num = 0\n",
    "# ========================================================\n",
    "            while agent_num == 0:\n",
    "                env.reset()\n",
    "                decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "                agent_num = len(decision_steps.obs[0])\n",
    "#                 print(agent_num)\n",
    "                \n",
    "            old_state = decision_steps.obs[0][0]\n",
    "#             print(old_state)\n",
    "            obs_space = len(old_state) # Check obs space         \n",
    "            EP_STEP = 0\n",
    "#             print(ep)\n",
    "            while EP_STEP < 100:\n",
    "                while len(decision_steps.reward) == 0:\n",
    "                    env.step() \n",
    "                    decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "                old_state = decision_steps.obs[0][0]\n",
    "                    \n",
    "                if obs_space > 0:\n",
    "\n",
    "                    action_values = net.activate(old_state)\n",
    "                    action = group_spec.action_spec.empty_action(agent_num)\n",
    "                    action[0] = action_values\n",
    "                \n",
    "                    env.set_actions(group_name, action)\n",
    "                    env.step()\n",
    "                    \n",
    "                    decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "\n",
    "                    if 0 in terminal_steps.agent_id:\n",
    "                        reward = terminal_steps.reward[0]\n",
    "                        accumulative_r += reward\n",
    "                        done = True\n",
    "                    else:\n",
    "                        reward = 0.1\n",
    "                        done = False\n",
    "                        accumulative_r += reward\n",
    "                    \n",
    "                    EP_STEP += 1\n",
    "#                         print(decision_steps.obs[0])\n",
    "                            \n",
    "                else:\n",
    "                    env.step()  \n",
    "                if done:\n",
    "                        break\n",
    "                \n",
    "            ep_r.append(accumulative_r)\n",
    "        genome.fitness = np.min(ep_r)/float(EP_STEP)    # depends on the minimum episode reward\n",
    "\n",
    "\n",
    "def run():\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation, CONFIG)\n",
    "    pop = neat.Population(config)\n",
    "    pop = neat.Checkpointer.restore_checkpoint('neat-checkpoint-%i' % CHECKPOINT)\n",
    "    # recode history\n",
    "    stats = neat.StatisticsReporter()\n",
    "    pop.add_reporter(stats)\n",
    "    pop.add_reporter(neat.StdOutReporter(True))\n",
    "    pop.add_reporter(neat.Checkpointer(10))\n",
    "\n",
    "    pop.run(eval_genomes, 1000)       # train 10 generations\n",
    "\n",
    "#     visualize training\n",
    "    visualize.plot_stats(stats, ylog=False, view=True)\n",
    "    visualize.plot_species(stats, view=True)\n",
    "\n",
    "\n",
    "def evaluation():\n",
    "    p = neat.Checkpointer.restore_checkpoint('neat-checkpoint-%i' % CHECKPOINT)\n",
    "    winner = p.run(eval_genomes, 1)     # find the winner in restored population\n",
    "\n",
    "    # show winner net\n",
    "    node_names = {-1: 'In0', -2: 'In1', -3: 'In3', -4: 'In4', 0: 'act1', 1: 'act2'}\n",
    "    visualize.draw_net(p.config, winner, True, node_names=node_names)\n",
    "\n",
    "    net = neat.nn.FeedForwardNetwork.create(winner, p.config)\n",
    "    while True:\n",
    "        agent_num = 0\n",
    "# ========================================================\n",
    "        while agent_num == 0:\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(group_name)\n",
    "            agent_num = len(decision_steps.obs[0])\n",
    "#                 print(agent_num)\n",
    "                \n",
    "            old_state = decision_steps.obs[0][0]\n",
    "            obs_space = len(old_state) # Check obs space       \n",
    "            \n",
    "        while True:\n",
    "            if obs_space > 0:\n",
    "                action_values = net.activate(old_state)\n",
    "                action = group_spec.action_spec.empty_action(agent_num)\n",
    "                action[0] = action_values\n",
    "\n",
    "                env.set_actions(group_name, action)\n",
    "                env.step()\n",
    "                if terminal_steps.reward.size>0:\n",
    "                    done = True\n",
    "                else:\n",
    "                    done = False\n",
    "            else:\n",
    "                env.step()    \n",
    "                \n",
    "            if done: break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if TRAINING:\n",
    "        run()\n",
    "    else:\n",
    "        evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
